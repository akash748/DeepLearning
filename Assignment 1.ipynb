{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a817d9b",
   "metadata": {},
   "source": [
    "Question 1: What is the function of a summation junction of a neuron? What is threshold activation function?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35eba6a5",
   "metadata": {},
   "source": [
    "In the summation, all features are multiplied by their weights and bias are summed up. (Y=W1X1+W2X2+b). This summed function is applied over an Activation function. The output from this neuron is multiplied with the weight W3 and supplied as input to the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4389eb1b",
   "metadata": {},
   "source": [
    "Threshold activation function is a type of function which activates the neuron above a certain threshold and deactivates it below a certain threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0624c233",
   "metadata": {},
   "source": [
    "<img src = \"image1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8249324c",
   "metadata": {},
   "source": [
    "Question 2: Explain the McCulloch–Pitts model of neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e606e1",
   "metadata": {},
   "source": [
    "The McCulloch-Pitts model was an extremely simple artificial neuron. The inputs could be either a zero or a one. And the output was a zero or a one. And each input could be either excitatory or inhibitory.\n",
    "\n",
    "Now the whole point was to sum the inputs. If an input is one, and is excitatory in nature, it added one. If it was one, and was inhibitory, it subtracted one from the sum. This is done for all inputs, and a final sum is calculated.\n",
    "\n",
    "Now, if this final sum is less than some value (which you decide, say T), then the output is zero. Otherwise, the output is a one.\n",
    "\n",
    "Here is a graphical representation of the McCulloch-Pitts model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb90aaf6",
   "metadata": {},
   "source": [
    "<img src = \"image2.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25c9e14",
   "metadata": {},
   "source": [
    "In the figure, I represented things with named variables. The variables w1, w2 and w3 indicate which input is excitatory, and which one is inhibitory. These are called \"weights\". So, in this model, if a weight is 1, it is an excitatory input. If it is -1, it is an inhibitory input.\n",
    "\n",
    "x1, x2, and x3 represent the inputs. There could be more (or less) inputs if required. And accordingly, there would be more 'w's to indicate if that particular input is excitatory or inhibitory.\n",
    "\n",
    "Now, if you think about it, you can calculate the sum using the 'x's and 'w's... something like this:\n",
    "\n",
    "sum = x1w1 + x2w2 + x3w3 + ...\n",
    "\n",
    "This is what is called a 'weighted sum'.\n",
    "\n",
    "Now that the sum has been calculated, we check if sum < T or not. If it is, then the output is made zero. Otherwise, it is made a one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c29c69",
   "metadata": {},
   "source": [
    "Question 3:Explain the ADALINE network model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b3c2dc",
   "metadata": {},
   "source": [
    "Adaline is a single-unit neuron, which receives input from several units and also from one unit, called bias. An Adeline model consists of trainable weights. The inputs are of two values (+1 or -1) and the weights have signs (positive or negative).\n",
    "\n",
    "Initially random weights are assigned. The net input calculated is applied to a quantizer transfer function (possibly activation function) that restores the output to +1 or -1. The Adaline model compares the actual output with the target output and with the bias and the adjusts all the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b12ee91",
   "metadata": {},
   "source": [
    "<img src = \"image3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce0f0c8",
   "metadata": {},
   "source": [
    "Training Algorithm\n",
    "\n",
    "\n",
    "The Adaline network training algorithm is as follows:\n",
    "\n",
    "\n",
    "Step0: weights and bias are to be set to some random values but not zero. Set the learning rate parameter α.\n",
    "\n",
    "\n",
    "Step1: perform steps 2-6 when stopping condition is false.\n",
    "\n",
    "\n",
    "Step2: perform steps 3-5 for each bipolar training pair s:t\n",
    "\n",
    "\n",
    "Step3: set activations foe input units i=1 to n.\n",
    "\n",
    "\n",
    "Step4: calculate the net input to the output unit.\n",
    "\n",
    "\n",
    "Step5: update the weight and bias for i=1 to n\n",
    "\n",
    "\n",
    "Step6: if the highest weight change that occurred during training is smaller than a specified tolerance then stop the training process, else continue. This is the test for the stopping condition of a network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0893778",
   "metadata": {},
   "source": [
    "Question 4:What is the constraint of a simple perceptron? Why it may fail with a real-world data set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe913d2",
   "metadata": {},
   "source": [
    "Perceptron networks have several limitations. First, the output values of a perceptron can take on only one of two values (0 or 1) due to the hard-limit transfer function. Second, perceptrons can only classify linearly separable sets of vectors. If a straight line or a plane can be drawn to separate the input vectors into their correct categories, the input vectors are linearly separable. If the vectors are not linearly separable, learning will never reach a point where all vectors are classified properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c2bbd5",
   "metadata": {},
   "source": [
    "Generally the real world datsets have more than one class and they are not linearly seperable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ac3e11",
   "metadata": {},
   "source": [
    "Question 5:What is linearly inseparable problem? What is the role of the hidden layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35887292",
   "metadata": {},
   "source": [
    "A set of input vectors (or a training set) will be said to be linearly non-separable if no hyperplane\n",
    "exists such that each vector lies on the pre-assigned side of the hyperplane"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38443085",
   "metadata": {},
   "source": [
    "Hidden layer(s) are the secret sauce of your network. They allow you to model complex data thanks to their nodes/neurons. They are “hidden” because the true values of their nodes are unknown in the training dataset. In fact, we only know the input and output. Each neural network has at least one hidden layer. Otherwise, it is not a neural network. Networks with multiple hidden layers are called deep neural networks. The most common type of hidden layer is the fully-connected layer. Here, each neuron is connected to all the others in two adjacent layers. It is not connected to the ones in the same layer. Convolutional layers are another type of hidden layers that are very prominent when dealing with images.\n",
    "\n",
    "Neurons are the processing units of the network. Each neuron weighs and sums the different inputs and passes them through an activation function. The role of the activation function is to buffer the data before it is fed to the next layer. You can change the activity of your neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd6c978",
   "metadata": {},
   "source": [
    "Question 6: Explain XOR problem in case of a simple perceptron."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29a9e32",
   "metadata": {},
   "source": [
    "XOR is where if one is 1 and other is 0 but not both.\n",
    "\n",
    "\n",
    "Need:\n",
    "    \n",
    "1.w1 + 0.w2 cause a fire, i.e. >= t\n",
    "\n",
    "0.w1 + 1.w2 >= t\n",
    "\n",
    "0.w1 + 0.w2 doesn't fire, i.e. < t\n",
    "\n",
    "1.w1 + 1.w2 also doesn't fire, < t\n",
    "\n",
    "\n",
    "w1 >= t\n",
    "\n",
    "w2 >= t\n",
    "\n",
    "0 < t\n",
    "\n",
    "w1+w2 < t\n",
    "\n",
    "Contradiction.\n",
    "\n",
    "Note: We need all 4 inequalities for the contradiction. If weights negative, e.g. weights = -4 and t = -5, then weights can be greater than t yet adding them is less than t, but t > 0 stops this.\n",
    "\n",
    "\n",
    "A \"single-layer\" perceptron can't implement XOR. The reason is because the classes in XOR are not linearly separable. You cannot draw a straight line to separate the points (0,0),(1,1) from the points (0,1),(1,0).\n",
    "\n",
    "\n",
    "Led to invention of multi-layer networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970d2094",
   "metadata": {},
   "source": [
    "Question 7:Design a multi-layer perceptron to implement A XOR B."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f85ac5",
   "metadata": {},
   "source": [
    "Step1: Now for the corresponding weight vector $\\boldsymbol{w} : (\\boldsymbol{w_{1}}, \\boldsymbol{w_{2}})$ of the input vector $\\boldsymbol{x} : (\\boldsymbol{x_{1}}, \\boldsymbol{x_{2}})$ to the AND and OR node, the associated Perceptron Function can be defined as:\n",
    "\n",
    "  \\[$\\boldsymbol{\\hat{y}_{1}} = \\Theta\\left(w_{1} x_{1}+w_{2} x_{2}+b_{AND}\\right)$ \\]\n",
    "  \n",
    "\n",
    "  \\[$\\boldsymbol{\\hat{y}_{2}} = \\Theta\\left(w_{1} x_{1}+w_{2} x_{2}+b_{OR}\\right)$ \\]\n",
    "  \n",
    "\n",
    "Step2: The output ($\\boldsymbol{\\hat{y}}_{1}$) from the AND node will be inputed to the NOT node with weight $\\boldsymbol{w_{NOT}}$ and the associated Perceptron Function can be defined as:\n",
    "\n",
    "  \\[$\\boldsymbol{\\hat{y}_{3}} = \\Theta\\left(w_{NOT}  \\boldsymbol{\\hat{y}_{1}}+b_{NOT}\\right)$\\]\n",
    "  \n",
    "\n",
    "Step3: The output ($\\boldsymbol{\\hat{y}}_{2}$) from the OR node and the output ($\\boldsymbol{\\hat{y}}_{3}$) from NOT node as mentioned in Step2 will be inputed to the AND node with weight $(\\boldsymbol{w_{AND1}}, \\boldsymbol{w_{AND2}})$. Then the corresponding output $\\boldsymbol{\\hat{y}}$ is the final output of the XOR logic function. The associated Perceptron Function can be defined as:\n",
    "\n",
    "  \\[$\\boldsymbol{\\hat{y}} = \\Theta\\left(w_{AND1}  \\boldsymbol{\\hat{y}_{3}}+w_{AND2}  \\boldsymbol{\\hat{y}_{2}}+b_{AND}\\right)$\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5126d994",
   "metadata": {},
   "source": [
    "<img src = \"image4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc3ad8a",
   "metadata": {},
   "source": [
    "Question 8: Explain the single-layer feed forward architecture of ANN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3098664",
   "metadata": {},
   "source": [
    "In this type of network, we have only two layers input layer and output layer but the input layer does not count because no computation is performed in this layer. The output layer is formed when different weights are applied on input nodes and the cumulative effect per node is taken. After this, the neurons collectively give the output layer to compute the output signals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e355ca46",
   "metadata": {},
   "source": [
    "<img src = \"image5.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909da58d",
   "metadata": {},
   "source": [
    "Question 9:Consider a multi-layer feed forward neural network. Enumerate and explain steps in the backpropagation algorithm used to train the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b9717a",
   "metadata": {},
   "source": [
    "Note: The weights are taken randomly.\n",
    "\n",
    "Input layer: i/p – [x1 x2] = [0 1]\n",
    "\n",
    "\n",
    "Here since it is the input layer only the input values are present.\n",
    "\n",
    "\n",
    "Hidden layer: z1 – [v11 v21 v01] = [0.6 -0.1 03]\n",
    "\n",
    "\n",
    "Here v11 refers to the weight of first input x1 on z1, v21 refers to the weight of second input x2 on z1 and v01 refers to the bias value on z1.\n",
    "\n",
    "\n",
    "z2 – [v12 v22 v02] = [-0.3 0.4 0.5]\n",
    "\n",
    "\n",
    "Here v12 refers to the weight of first input x1 on z2, v22 refers to the weight of second input x2 on z2 and v02 refers to the bias value on z2.\n",
    "\n",
    "\n",
    "Output layer: yin – [w11 w21 w01] = [0.4 0.1 -0.2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Here w11 refers to the weight of first neuron z1 in a hidden layer on yin, w21 refers to the weight of second neuron z2 in a hidden layer on yin and w01 refers to the bias value on yin. Let’s consider three variables, k which refers to the neurons in the output layer, ‘j’ which refers to the neurons in the hidden layer and ‘i’ which refers to the neurons in the input layer.\n",
    "\n",
    "\n",
    "Therefore, \n",
    "\n",
    "\n",
    "k = 1\n",
    "\n",
    "\n",
    "j = 1, 2(meaning first neuron and second neuron in hidden layer)\n",
    "\n",
    "\n",
    "i = 1, 2(meaning first and second neuron in the input layer)\n",
    "\n",
    "\n",
    "Below are some conditions to be followed in BPNs.\n",
    "\n",
    "\n",
    "Conditions/Constraints:\n",
    "\n",
    "In BPN, the activation function used should be differentiable.\n",
    "\n",
    "The input for bias is always 1.\n",
    "\n",
    "To proceed with the problem, let:\n",
    "\n",
    "\n",
    "Target value, t = 1\n",
    "\n",
    "\n",
    "Learning rate, α = 0.25\n",
    "\n",
    "\n",
    "Activation function = Binary sigmoid function\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Binary sigmoid function, f(x) = (1+e-x)-1       eq. (1)\n",
    "\n",
    "\n",
    "And, f'(x) = f(x)[1-f(x)]       eq. (2)\n",
    "\n",
    "\n",
    "There are three steps to solve the problem:\n",
    "\n",
    "\n",
    "Computing the output, y.\n",
    "\n",
    "Backpropagation of errors, i.e., between output and hidden layer, hidden and input layer.\n",
    "Updating weights.\n",
    "\n",
    "Step 1:\n",
    "The value y is calculated by finding yin and applying the activation function.\n",
    "\n",
    "\n",
    "yin is calculated as:\n",
    "\n",
    "\n",
    "yin = w01 + z1*w11 + z2*w21      eq. (3)\n",
    "\n",
    "\n",
    "Here, z1 and z2 are the values from hidden layer, calculated by finding zin1, zin2 and applying activation function to them.\n",
    "\n",
    "zin1 and zin2 are calculated as:\n",
    "\n",
    "\n",
    "zin1 = v01 + x1*v11 + x2*v21   eq. (4)\n",
    "\n",
    "\n",
    "zin2 = v02 + x1*v12 + x2*v22   eq. (5)\n",
    "\n",
    "\n",
    "From (4)\n",
    "\n",
    "\n",
    "zin1 = 0.3 + 0*0.6 + 1*(-0.1)\n",
    "\n",
    "\n",
    "zin1 = 0.2\n",
    "\n",
    "\n",
    "z1 = f(zin1) = (1+e-0.2)-1  From (1)\n",
    "\n",
    "\n",
    "z1 = 0.5498\n",
    "\n",
    "\n",
    "From (5)\n",
    "\n",
    "\n",
    "zin2 = 0.5 + 0*(-0.3) + 1*0.4\n",
    "\n",
    "\n",
    "zin2 = 0.9\n",
    "\n",
    "\n",
    "z2 = f(zin2) = (1+e-0.9)-1    From (1)\n",
    "\n",
    "z2 = 0.7109\n",
    "\n",
    "\n",
    "\n",
    "From (3)\n",
    "\n",
    "\n",
    "yin = (-0.2) + 0.5498*0.4 + 0.7109*0.1\n",
    "\n",
    "yin = 0.0910\n",
    "\n",
    "y = f(yin) = (1+e-0.0910)-1     From (1)\n",
    "\n",
    "y = 0.5227\n",
    "\n",
    "Here, y is not equal to the target ‘t’, which is 1. And we proceed to calculate the errors and then update weights from them in order to achieve the target value.\n",
    "\n",
    "\n",
    "Step 2:\n",
    "\n",
    "(a) Calculating the error between output and hidden layer\n",
    "\n",
    "Error between output and hidden layer is represented as δk, where k represents the neurons in output layer as mentioned above. The error is calculated as:\n",
    "\n",
    "\n",
    "δk = (tk – yk) * f'(yink)               eq. (6)\n",
    "\n",
    "\n",
    "where, f'(yink) = f(yink)[1 – f(yink)]        From (2)\n",
    "\n",
    "\n",
    "Since k = 1 (Assumed above),\n",
    "\n",
    "\n",
    "δ = (t – y) f'(yin)                       eq. (7)\n",
    "\n",
    "\n",
    "where, f'(yin) = f(yin)[1 – f(yin)]\n",
    "\n",
    "\n",
    "f'(yin) = 0.5227[1 – 0.5227]\n",
    "\n",
    "\n",
    "f'(yin) = 0.2495\n",
    "\n",
    "\n",
    "Therefore, \n",
    "\n",
    "\n",
    "δ = (1 – 0.5227) * 0.2495           From (7)\n",
    "\n",
    "\n",
    "δ = 0.1191, is the error\n",
    "\n",
    "\n",
    "Note: (Target – Output) i.e., (t – y) is the error in the output not in the layer. Error in a layer is contributed by different factors like weights and bias.\n",
    "\n",
    "(b) Calculating the error between hidden and input layer.\n",
    "Error between hidden and input layer is represented as δj, where j represents the number of neurons in the hidden layer as mentioned above. The error is calculated as:\n",
    "\n",
    "δj = δinj  * f'(zinj)            eq. (8)\n",
    "\n",
    "where, \n",
    "\n",
    "\n",
    "\n",
    "δinj = ∑k=1 to n (δk * wjk)    eq. (9)\n",
    "\n",
    "f'(zinj) = f(zinj)[1 – f(zinj)]    eq. (10)\n",
    "\n",
    "Since k = 1(Assumed above) eq. (9) becomes:\n",
    "\n",
    "δinj = δ * wj1       eq. (11)\n",
    "\n",
    "As j = 1, 2, we will have one error values for each neuron and total of 2 errors values.\n",
    "\n",
    "δ1 = δin1  * f'(zin1)        eq. (12),  From (8)\n",
    "\n",
    "δin1 = δ * w11                 From (11)\n",
    "\n",
    "δin1 = 0.1191 * 0.4        From weights vectors\n",
    "\n",
    "δin1 = 0.04764               \n",
    "\n",
    "f'(zin1) = f(zin1)[1 – f(zin1)]\n",
    "\n",
    "f'(zin1) = 0.5498[1 – 0.5498]         As f(zin1) = z1\n",
    "\n",
    "f'(zin1) = 0.2475\n",
    "\n",
    "Substituting in (12)\n",
    "\n",
    "δ1 = 0.04674 * 0.2475 = 0.0118\n",
    "\n",
    "δ2 = δin2  * f'(zin2)        eq. (13), From (8)\n",
    "\n",
    "δin2 = δ * w21                 From (11)\n",
    "\n",
    "δin2 = 0.1191 * 0.1        From weights vectors\n",
    "\n",
    "δin2 = 0.0119              \n",
    "\n",
    "f'(zin2) = f(zin2)[1 – f(zin2)]\n",
    "\n",
    "f'(zin2) = 0.7109[1 – 0.7109]         As f(zin2) = z2\n",
    "\n",
    "f'(zin2) = 0.2055\n",
    "\n",
    "Substituting in (13)\n",
    "\n",
    "δ2 = 0.0119 * 0.2055 = 0.00245\n",
    "\n",
    "The errors have been calculated, the weights have to be updated using these error values.\n",
    "\n",
    "Step 3:\n",
    "The formula for updating weights for output layer is:\n",
    "\n",
    "wjk(new) = wjk(old) + Δwjk         eq. (14)\n",
    "\n",
    "where, Δwjk = α * δk * zj           eq. (15)\n",
    "\n",
    "Since k = 1, (15) becomes: \n",
    "\n",
    "Δwjk = α * δ * zi                        eq. (16)                             \n",
    "\n",
    "The formula for updating weights for hidden layer is:\n",
    "\n",
    "\n",
    "\n",
    "vij(new) = vij(old) + Δvij           eq. (17)\n",
    "\n",
    "where, Δvi = α * δj * xi           eq. (18)\n",
    "\n",
    "From (14) and (16)\n",
    "\n",
    "w11(new) = w11(old) + Δw11 = 0.4 + α * δ * z1  =  0.4 + 0.25 * 0.1191 * 0.5498 = 0.4164\n",
    "\n",
    "w21(new) = w21(old) + Δw21 = 0.1 + α * δ * z2 = 0.1 + 0.25 * 0.1191 * 0.7109 = 0.12117\n",
    "\n",
    "w01(new) = w01(old) + Δw01 = (-0.2) + α * δ * bias = (-0.2) + 0.25 * 0.1191 * 1 = -0.1709, kindly note the 1 taken here is input considered for bias as per the conditions.\n",
    "\n",
    "These are the updated weights of the output layer.\n",
    "\n",
    "From (17) and (18)\n",
    "\n",
    "v11(new) = v11(old) + Δv11 = 0.6 +  α * δ1 * x1 = 0.6 + 0.25 * 0.0118 * 0 = 0.6\n",
    "\n",
    "v21(new) = v21(old) + Δv21 = (-0.1) +  α * δ1 * x2 = (-0.1) + 0.25 * 0.0118 * 1 = 0.00295\n",
    "\n",
    "v01(new) = v01(old) + Δv01 = 0.3 +  α * δ1 * bias = 0.3 + 0.25 * 0.0118 * 1 = 0.00295, kindly note the 1 taken here is input considered for bias as per the conditions.\n",
    "\n",
    "v12(new) = v12(old) + Δv12 = (-0.3) +  α * δ2 * x1 = (-0.3) + 0.25 * 0.00245 * 0 = -0.3\n",
    "\n",
    "v22(new) = v22(old) + Δv22 = 0.4 +  α * δ2 * x2 = 0.4 + 0.25 * 0.00245 * 1 = 0.400612\n",
    "\n",
    "v02(new) = v02(old) + Δv02 = 0.5 +  α * δ2 * bias = 0.5 + 0.25 * 0.00245 * 1 = 0.500612, kindly note the 1 taken here is input considered for bias as per the conditions.\n",
    "\n",
    "These are all the updated weights of the hidden layer.\n",
    "\n",
    "These three steps are repeated until the output ‘y’ is equal to the target ‘t’.\n",
    "\n",
    "This is how the BPNs work. The backpropagation in BPN refers to that the error in the present layer is used to update weights between the present and previous layer by backpropagating the error values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcb3c12",
   "metadata": {},
   "source": [
    "Question 10:Explain the competitive network architecture of ANN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fa01da",
   "metadata": {},
   "source": [
    "A neural network has many layers and each layer performs a specific function, and as the complexity of the model increases, the number of layers also increases that why it is known as the multi-layer perceptron.\n",
    "\n",
    "The purest form of a neural network has three layers input layer, the hidden layer, and the output layer. The input layer picks up the input signals and transfers them to the next layer and finally, the output layer gives the final prediction and these neural networks have to be trained with some training data as well like machine learning algorithms before providing a particular problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9215a336",
   "metadata": {},
   "source": [
    "<img src = \"image6.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3fff9c",
   "metadata": {},
   "source": [
    "Question 11:What are the advantages and disadvantages of neural networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762159da",
   "metadata": {},
   "source": [
    "Here are some advantages of Artificial Neural Networks ( ANN)\n",
    "\n",
    "\n",
    "Storing information on the entire network: Information such as in traditional programming is stored on the entire network, not on a database. The disappearance of a few pieces of information in one place does not restrict the network from functioning.\n",
    "\n",
    "\n",
    "The ability to work with inadequate knowledge: After ANN training, the data may produce output even with incomplete information. The lack of performance here depends on the importance of the missing information. \n",
    "\n",
    "\n",
    "It has fault tolerance:  Corruption of one or more cells of ANN does not prevent it from generating output. This feature makes the networks fault-tolerant.\n",
    "\n",
    "\n",
    "Having a distributed memory: For ANN to be able to learn, it is necessary to determine the examples and to teach the network according to the desired output by showing these examples to the network. The network's progress is directly proportional to the selected instances, and if the event can not be shown to the network in all its aspects, the network can produce incorrect output \n",
    "\n",
    "\n",
    "Gradual corruption:  A network slows over time and undergoes relative degradation. The network problem does not immediately corrode.\n",
    "\n",
    "\n",
    "Ability to train machine: Artificial neural networks learn events and make decisions by commenting on similar events.\n",
    "\n",
    "\n",
    " Parallel processing ability:  Artificial neural networks have numerical strength that can perform more than one job at the same time. \n",
    " \n",
    "\n",
    "Disadvantages of Artificial Neural Networks (ANN)\n",
    "\n",
    "\n",
    "Hardware dependence:  Artificial neural networks require processors with parallel processing power, by their structure. For this reason, the realization of the equipment is dependent.\n",
    "\n",
    "\n",
    "Unexplained functioning of the network: This is the most important problem of ANN. When ANN gives a probing solution, it does not give a clue as to why and how. This reduces trust in the network.\n",
    "\n",
    "\n",
    "Assurance of proper network structure:  There is no specific rule for determining the structure of artificial neural networks. The appropriate network structure is achieved through experience and trial and error. \n",
    "\n",
    "\n",
    "The difficulty of showing the problem to the network:  ANNs can work with numerical information. Problems have to be translated into numerical values before being introduced to ANN. The display mechanism to be determined here will directly influence the performance of the network. This depends on the user's ability. \n",
    "\n",
    "\n",
    "The duration of the network is unknown: The network is reduced to a certain value of the error on the sample means that the training has been completed. This value does not give us optimum results. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed4a86a",
   "metadata": {},
   "source": [
    "Question 12: Write short notes on ReLU function and Gradient Descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01df595",
   "metadata": {},
   "source": [
    "The ReLU function is the Rectified linear unit. It is the most widely used activation function. It is defined as:\n",
    "\n",
    " f(x) = \\max(0, x) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c38dfb1",
   "metadata": {},
   "source": [
    "<img src = \"image7.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a352bd9b",
   "metadata": {},
   "source": [
    "Gradient descent is an optimization algorithm which is commonly-used to train machine learning models and neural networks.  Training data helps these models learn over time, and the cost function within gradient descent specifically acts as a barometer, gauging its accuracy with each iteration of parameter updates. Until the function is close to or equal to zero, the model will continue to adjust its parameters to yield the smallest possible error. Once machine learning models are optimized for accuracy, they can be powerful tools for artificial intelligence (AI) and computer science applications. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55132759",
   "metadata": {},
   "source": [
    "<img src = \"image8.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1834bf9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

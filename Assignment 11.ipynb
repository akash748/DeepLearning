{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "603046cb",
   "metadata": {},
   "source": [
    "Question 1: Write the Python code to implement a single neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3abda721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all necessary libraries\n",
    "from numpy import exp, array, random, dot, tanh\n",
    " \n",
    "# Class to create a neural\n",
    "# network with single neuron\n",
    "class NeuralNetwork():\n",
    "     \n",
    "    def __init__(self):\n",
    "         \n",
    "        # Using seed to make sure it'll \n",
    "        # generate same weights in every run\n",
    "        random.seed(1)\n",
    "         \n",
    "        # 3x1 Weight matrix\n",
    "        self.weight_matrix = 2 * random.random((3, 1)) - 1\n",
    "        # tanh as activation function\n",
    "    def tanh(self, x):\n",
    "        return tanh(x)\n",
    " \n",
    "    # derivative of tanh function.\n",
    "    # Needed to calculate the gradients.\n",
    "    def tanh_derivative(self, x):\n",
    "        return 1.0 - tanh(x) ** 2\n",
    " \n",
    "    # forward propagation\n",
    "    def forward_propagation(self, inputs):\n",
    "        return self.tanh(dot(inputs, self.weight_matrix))\n",
    "     \n",
    "    # training the neural network.\n",
    "    def train(self, train_inputs, train_outputs,\n",
    "                            num_train_iterations):\n",
    "                                 \n",
    "        # Number of iterations we want to\n",
    "        # perform for this set of input.\n",
    "        for iteration in range(num_train_iterations):\n",
    "            output = self.forward_propagation(train_inputs)\n",
    " \n",
    "            # Calculate the error in the output.\n",
    "            error = train_outputs - output\n",
    "        # multiply the error by input and then\n",
    "            # by gradient of tanh function to calculate\n",
    "            # the adjustment needs to be made in weights\n",
    "            adjustment = dot(train_inputs.T, error *\n",
    "                             self.tanh_derivative(output))\n",
    "                              \n",
    "            # Adjust the weight matrix\n",
    "            self.weight_matrix += adjustment\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8f383c",
   "metadata": {},
   "source": [
    "Question 2: Write the Python code to implement ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b13737fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(input):\n",
    "    return max(0,input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abc8e3d",
   "metadata": {},
   "source": [
    "Question 3: Write the Python code for a dense layer in terms of matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ee2454c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul(a,b):\n",
    "    ar,ac = a.shape # n_rows * n_cols\n",
    "    br,bc = b.shape\n",
    "    assert ac==br\n",
    "    c = torch.zeros(ar, bc)\n",
    "    for i in range(ar):\n",
    "        for j in range(bc):\n",
    "            for k in range(ac): c[i,j] += a[i,k] * b[k,j]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14e4e4a",
   "metadata": {},
   "source": [
    "Questiion 4: What is the “hidden size” of a layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c39ff0",
   "metadata": {},
   "source": [
    "The number of neurons present in a layer constitute to the hidden size of the layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba58859",
   "metadata": {},
   "source": [
    "Question 5: What does the t method do in PyTorch?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531d492f",
   "metadata": {},
   "source": [
    "Expects input to be <= 2-D tensor and transposes dimensions 0 and 1.\n",
    "\n",
    "0-D and 1-D tensors are returned as is. When input is a 2-D tensor this is equivalent to transpose(input, 0, 1).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184f4dc1",
   "metadata": {},
   "source": [
    "Question 6: Why is matrix multiplication written in plain Python very slow?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590692e0",
   "metadata": {},
   "source": [
    "PyTorch didn't write its matrix multiplication in Python, but rather in C++ to make it fast. In general, whenever we do computations on tensors we will need to vectorize them so that we can take advantage of the speed of PyTorch, usually by using two techniques: elementwise arithmetic and broadcasting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4303d68",
   "metadata": {},
   "source": [
    "Question 7: In matmul, why is ac==br?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a24e30",
   "metadata": {},
   "source": [
    "In maths it is a rule for a matrix multiplicatiion to be possible:\n",
    "\n",
    "No of columns of matrix A = No of rows of matrix B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff33d93f",
   "metadata": {},
   "source": [
    "Question 8: In Jupyter Notebook, how do you measure the time taken for a single cell to execute?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8f1db6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "m1 = torch.randn(5,28*28)\n",
    "m2 = torch.randn(784,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2c0d47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 611 ms\n"
     ]
    }
   ],
   "source": [
    "%time t1=matmul(m1, m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1046752e",
   "metadata": {},
   "source": [
    "Here is the magic command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0ca384",
   "metadata": {},
   "source": [
    "Question 9: What is elementwise arithmetic?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d001502",
   "metadata": {},
   "source": [
    "All the basic operators (+, -, *, /, >, <, ==) can be applied elementwise. That means if we write a+b for two tensors a and b that have the same shape, we will get a tensor composed of the sums the elements of a and b:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74bd5757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12., 14.,  3.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import tensor\n",
    "a = tensor([10., 6, -4])\n",
    "b = tensor([2., 8, 7])\n",
    "a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4681c7d9",
   "metadata": {},
   "source": [
    "Question 10: Write the PyTorch code to test whether every element of a is greater than the corresponding element of b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18b1a1e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a>b).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5664cc77",
   "metadata": {},
   "source": [
    "We can use the .all() function to get our desired output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc1a778",
   "metadata": {},
   "source": [
    "Question 11:What is a rank-0 tensor? How do you convert it to a plain Python data type?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac931b2",
   "metadata": {},
   "source": [
    "Reduction operations like all(), sum() and mean() return tensors with only one element, called rank-0 tensors. If you want to convert this to a plain Python Boolean or number, you need to call .item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6934686b",
   "metadata": {},
   "source": [
    "Question 12: How does elementwise arithmetic help us speed up matmul?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb63d3d",
   "metadata": {},
   "source": [
    "With elementwise arithmetic, we can remove one of our three nested loops: we can multiply the tensors that correspond to the i-th row of a and the j-th column of b before summing all the elements, which will speed things up because the inner loop will now be executed by PyTorch at C speed.\n",
    "\n",
    "To access one column or row, we can simply write a[i,:] or b[:,j]. The : means take everything in that dimension. We could restrict this and take only a slice of that particular dimension by passing a range, like 1:5, instead of just :. In that case, we would take the elements in columns or rows 1 to 4 (the second number is noninclusive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b880c3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul(a,b):\n",
    "    ar,ac = a.shape\n",
    "    br,bc = b.shape\n",
    "    assert ac==br\n",
    "    c = torch.zeros(ar, bc)\n",
    "    for i in range(ar):\n",
    "        for j in range(bc): c[i,j] = (a[i] * b[:,j]).sum()\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5df21940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.32 ms\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(5,28*28)\n",
    "b = torch.randn(784,10)\n",
    "%time t1=matmul(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1567160",
   "metadata": {},
   "source": [
    "The execution time reduced drastically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c3ae2b",
   "metadata": {},
   "source": [
    "Question 13: What are the broadcasting rules?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec596aa",
   "metadata": {},
   "source": [
    " Broadcasting is a term introduced by the NumPy library that describes how tensors of different ranks are treated during arithmetic operations. For instance, it's obvious there is no way to add a 3×3 matrix with a 4×5 matrix, but what if we want to add one scalar (which can be represented as a 1×1 tensor) with a matrix? Or a vector of size 3 with a 3×4 matrix? In both cases, we can find a way to make sense of this operation.\n",
    "\n",
    "Broadcasting gives specific rules to codify when shapes are compatible when trying to do an elementwise operation, and how the tensor of the smaller shape is expanded to match the tensor of the bigger shape. It's essential to master those rules if you want to be able to write code that executes quickly. In this section, we'll expand our previous treatment of broadcasting to understand these rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6510d658",
   "metadata": {},
   "source": [
    "When operating on two tensors, PyTorch compares their shapes elementwise. It starts with the trailing dimensions and works its way backward, adding 1 when it meets empty dimensions. Two dimensions are compatible when one of the following is true:\n",
    "\n",
    "They are equal.\n",
    "One of them is 1, in which case that dimension is broadcast to make it the same as the other."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bad73866",
   "metadata": {},
   "source": [
    "Question 1: How does unsqueeze help us to solve certain broadcasting problems?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2564ae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 3]), torch.Size([3, 1]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import tensor\n",
    "c = tensor([10.,20,30])\n",
    "m = tensor([[1., 2, 3], [4,5,6], [7,8,9]])\n",
    "c = c.unsqueeze(1)\n",
    "m.shape,c.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a929c1",
   "metadata": {},
   "source": [
    "If we want to broadcast in the other dimension, we have to change the shape of our vector to make it a 3×1 matrix. This is done with the unsqueeze method in PyTorch:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7960cf70",
   "metadata": {},
   "source": [
    "Question 2: How can we use indexing to do the same operation as unsqueeze?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90762fc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3]), torch.Size([1, 3]), torch.Size([3, 1]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = tensor([10.,20,30])\n",
    "c.shape, c[None,:].shape,c[:,None].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521e9228",
   "metadata": {},
   "source": [
    "Question 3: When adding a vector of size 3 to a matrix of size 3×3, are the elements of the vector added to each row or each column of the matrix? (Be sure to check your answer by running this code in a notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04966b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[11., 22., 33.],\n",
       "        [14., 25., 36.],\n",
       "        [17., 28., 39.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = tensor([10.,20,30])\n",
    "m = tensor([[1., 2, 3], [4,5,6], [7,8,9]])\n",
    "c+m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3518b8c",
   "metadata": {},
   "source": [
    "Question 4: Implement matmul using Einstein summation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d368285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul(a,b): return torch.einsum('ik,kj->ij', a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2218e915",
   "metadata": {},
   "source": [
    "Einstein summation is a very practical way of expressing operations involving indexing and sum of products. Note that you can have just one member on the lefthand side. For instance, this:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fa9274",
   "metadata": {},
   "source": [
    "Question 5: What does a repeated index letter represent on the lefthand side of einsum?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e4a001",
   "metadata": {},
   "source": [
    "Repeated indices on the left side are implicitly summed over if they are not on the right side"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9142490a",
   "metadata": {},
   "source": [
    "Question 6: What are the three rules of Einstein summation notation? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd290d0",
   "metadata": {},
   "source": [
    "The rules of Einstein summation notation are as follows:\n",
    "\n",
    "Repeated indices on the left side are implicitly summed over if they are not on the right side.\n",
    "Each index can appear at most twice on the left side.\n",
    "The unrepeated indices on the left side must appear on the right side."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947f25fa",
   "metadata": {},
   "source": [
    "Question 7: What are the forward pass and backward pass of a neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e21081",
   "metadata": {},
   "source": [
    "As we saw in <<chapter_mnist_basics>>, to train a model, we will need to compute all the gradients of a given loss with respect to its parameters, which is known as the *backward pass*. The *forward pass* is where we compute the output of the model on a given input, based on the matrix products. As we define our first neural net, we will also delve into the problem of properly initializing the weights, which is crucial for making training start properly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bcf9b8",
   "metadata": {},
   "source": [
    "Question 8: Why do we need to store some of the activations calculated for intermediate layers in the forward pass?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5616593",
   "metadata": {},
   "source": [
    "We need to store the activation functions for backpropogation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0ba9ea",
   "metadata": {},
   "source": [
    "Question 9: What is the downside of having activations with a standard deviation too far away from 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac903e3",
   "metadata": {},
   "source": [
    "For Example, the mean is close to zero, which is understandable since both our input and weight matrices have means close to zero. But the standard deviation, which represents how far away our activations go from the mean, went from 1 to 10. This is a really big problem because that's with just one layer. Modern neural nets can have hundred of layers, so if each of them multiplies the scale of our activations by 10, by the end of the last layer we won't have numbers representable by a computer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faecbf86",
   "metadata": {},
   "source": [
    "Question 10: How can weight initialization help avoid this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663a691a",
   "metadata": {},
   "source": [
    "Weight Initialization can help to reduce the randomly assigned weights in such a way that the output of the activations  has std close to 1 and we get uniform weights throughout"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "586537e4",
   "metadata": {},
   "source": [
    "Question 1: Describe the structure of an artificial neuron. How is it similar to a biological neuron? What are its main components?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335e2237",
   "metadata": {},
   "source": [
    "An artificial neuron is a connection point in an artificial neural network. Artificial neural networks, like the human body's biological neural network, have a layered architecture and each network node (connection point) has the capability to process input and forward output to other nodes in the network. In both artificial and biological architectures, the nodes are called neurons and the connections are characterized by synaptic weights, which represent the significance of the connection. As new data is received and processed, the synaptic weights change and this is how learning occurs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6cbce3",
   "metadata": {},
   "source": [
    "Main Components of Neuron are:\n",
    "\n",
    "Input, Nodes, Weights, Output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268d4edf",
   "metadata": {},
   "source": [
    "Question 2:What are the different types of activation functions popularly used? Explain each of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de826de",
   "metadata": {},
   "source": [
    "Linear Function:\n",
    "Equation : Linear function has the equation similar to as of a straight line i.e. y = ax\n",
    "No matter how many layers we have, if all are linear in nature, the final activation function of last layer is nothing but just a linear function of the input of first layer.\n",
    "\n",
    "Range : -inf to +inf\n",
    "\n",
    "Uses : Linear activation function is used at just one place i.e. output layer.\n",
    "\n",
    "Issues : If we will differentiate linear function to bring non-linearity, result will no more depend on input â€œxâ€ and function will become constant, it wonâ€™t introduce any ground-breaking behavior to our algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc38340",
   "metadata": {},
   "source": [
    "Sigmoid Function:\n",
    "A = 1/(1 + e-x)\n",
    "\n",
    "Nature : Non-linear. Notice that X values lies between -2 to 2, Y values are very steep. This means, small changes in x would also bring about large changes in the value of Y.\n",
    "\n",
    "Value Range : 0 to 1\n",
    "\n",
    "Uses : Usually used in output layer of a binary classification, where result is either 0 or 1, as value for sigmoid function lies between 0 and 1 only so, result can be predicted easily to be 1 if value is greater than 0.5 and 0 otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccdc934",
   "metadata": {},
   "source": [
    "Tanh Function:\n",
    "The activation that works almost always better than sigmoid function is Tanh function also knows as Tangent Hyperbolic function. Itâ€™s actually mathematically shifted version of the sigmoid function. Both are similar and can be derived from each other.\n",
    "\n",
    "\n",
    "Equation :-\n",
    "\n",
    "f(x) = tanh(x) = 2/(1 + e-2x) - 1\n",
    "\n",
    "OR\n",
    "\n",
    "tanh(x) = 2 * sigmoid(2x) - 1 \n",
    "\n",
    "Value Range :- -1 to +1\n",
    "\n",
    "Nature :- non-linear\n",
    "\n",
    "Uses :- Usually used in hidden layers of a neural network as itâ€™s values lies between -1 to 1 hence the mean for the hidden layer comes out be 0 or very close to it, hence helps in centering the data by bringing mean close to 0. This makes learning for the next layer much easier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f38b119",
   "metadata": {},
   "source": [
    "RELU:\n",
    "Stands for Rectified linear unit. It is the most widely used activation function. Chiefly implemented in hidden layers of Neural network.\n",
    "\n",
    "\n",
    "Equation :- A(x) = max(0,x). It gives an output x if x is positive and 0 otherwise.\n",
    "\n",
    "Value Range :- [0, inf)\n",
    "\n",
    "Nature :- non-linear, which means we can easily backpropagate the errors and have multiple layers of neurons being activated by the ReLU function.\n",
    "\n",
    "Uses :- ReLu is less computationally expensive than tanh and sigmoid because it involves simpler mathematical operations. At a time only a few neurons are activated making the network sparse making it efficient and easy for computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c600c8c4",
   "metadata": {},
   "source": [
    "Softmax Function:\n",
    "The softmax function is also a type of sigmoid function but is handy when we are trying to handle classification problems.\n",
    "\n",
    "\n",
    "Nature :- non-linear\n",
    "\n",
    "Uses :- Usually used when trying to handle multiple classes. The softmax function would squeeze the outputs for each class between 0 and 1 and would also divide by the sum of the outputs.\n",
    "\n",
    "Output:- The softmax function is ideally used in the output layer of the classifier where we are actually trying to attain the probabilities to define the class of each input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea06e96",
   "metadata": {},
   "source": [
    "Question 3: Explain, in details, Rosenblattâ€™s perceptron model. How can a set of data be classified using a simple perceptron?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c137449f",
   "metadata": {},
   "source": [
    "Rosenblatt perceptron is a binary single neuron model. The inputs integration is implemented through the addition of the weighted inputs that have fixed weights obtained during the training stage. If the result of this addition is larger than a given threshold Î¸ the neuron fires. When the neuron fires its output is set to 1, otherwise itâ€™s set to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dcec45",
   "metadata": {},
   "source": [
    "The perceptron is a simplified model of the real neuron that attempts to imitate it by the following process: it takes the input signals, letâ€™s call them x1, x2, â€¦, xn, computes a weighted sum z of those inputs, then passes it through a threshold function Ï• and outputs the result.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcf8b25",
   "metadata": {},
   "source": [
    "Here is a geometrical representation of this using only 2 inputs x1 and x2, so that we can plot it in 2 dimensions:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5acffad",
   "metadata": {},
   "source": [
    "<img src = \"image9.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040d4732",
   "metadata": {},
   "source": [
    "As you see above, the decision boundary of a perceptron with 2 inputs is a line. If there were 3 inputs, the decision boundary would be a 2D plane. In general, if we have n inputs the decision boundary will be a n-1 dimensional object called a hyperplane that separates our n-dimensional feature space into 2 parts: one in which the points are classified as positive, and one in which the points are classified as negative(by convention, we will consider points that are exactly on the decision boundary as being negative). Hence the perceptron is a binary classifier that is linear in terms of its weights.\n",
    "In the image above wâ€™ represents the weights vector without the bias term w0. wâ€™ has the property that it is perpendicular to the decision boundary and points towards the positively classified points. This vector determines the slope of the decision boundary, and the bias term w0 determines the offset of the decision boundary along the wâ€™ axis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb42cc8",
   "metadata": {},
   "source": [
    "Question 4:Explain the basic structure of a multi-layer perceptron. Explain how it can solve the XOR problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f3d330",
   "metadata": {},
   "source": [
    "Multilayer perceptrons, also known as feedforward neural networks having two or more layers have a higher processing power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd323595",
   "metadata": {},
   "source": [
    "Step1: Now for the corresponding weight vector  ğ’˜:(ğ’˜1,ğ’˜2)  of the input vector  ğ’™:(ğ’™1,ğ’™2)  to the AND and OR node, the associated Perceptron Function can be defined as:\n",
    "\n",
    "[ ğ’šÌ‚ 1=Î˜(ğ‘¤1ğ‘¥1+ğ‘¤2ğ‘¥2+ğ‘ğ´ğ‘ğ·)  ]\n",
    "\n",
    "[ ğ’šÌ‚ 2=Î˜(ğ‘¤1ğ‘¥1+ğ‘¤2ğ‘¥2+ğ‘ğ‘‚ğ‘…)  ]\n",
    "\n",
    "Step2: The output ( ğ’šÌ‚ 1 ) from the AND node will be inputed to the NOT node with weight  ğ’˜ğ‘µğ‘¶ğ‘»  and the associated Perceptron Function can be defined as:\n",
    "\n",
    "[ ğ’šÌ‚ 3=Î˜(ğ‘¤ğ‘ğ‘‚ğ‘‡ğ’šÌ‚ 1+ğ‘ğ‘ğ‘‚ğ‘‡) ]\n",
    "\n",
    "Step3: The output ( ğ’šÌ‚ 2 ) from the OR node and the output ( ğ’šÌ‚ 3 ) from NOT node as mentioned in Step2 will be inputed to the AND node with weight  (ğ’˜ğ‘¨ğ‘µğ‘«1,ğ’˜ğ‘¨ğ‘µğ‘«2) . Then the corresponding output  ğ’šÌ‚   is the final output of the XOR logic function. The associated Perceptron Function can be defined as:\n",
    "\n",
    "[ ğ’šÌ‚ =Î˜(ğ‘¤ğ´ğ‘ğ·1ğ’šÌ‚ 3+ğ‘¤ğ´ğ‘ğ·2ğ’šÌ‚ 2+ğ‘ğ´ğ‘ğ·) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1ee91c",
   "metadata": {},
   "source": [
    "<img src = \"image4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06becb40",
   "metadata": {},
   "source": [
    "Question 5:What is artificial neural network (ANN)? Explain some of the salient highlights in the different architectural options for ANN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6477eed",
   "metadata": {},
   "source": [
    "An ANN is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal to other neurons. An artificial neuron receives a signal then processes it and can signal neurons connected to it. The \"signal\" at a connection is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs. The connections are called edges. Neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Neurons may have a threshold such that a signal is sent only if the aggregate signal crosses that threshold. Typically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer), to the last layer (the output layer), possibly after traversing the layers multiple times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8249e919",
   "metadata": {},
   "source": [
    "There exist five basic types of neuron connection architecture : \n",
    "\n",
    "Single-layer feed-forward network\n",
    "\n",
    "Multilayer feed-forward network\n",
    "\n",
    "Single node with its own feedback\n",
    "\n",
    "Single-layer recurrent network\n",
    "\n",
    "Multilayer recurrent network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c3640d",
   "metadata": {},
   "source": [
    "Question 6:Explain the learning process of an ANN. Explain, with example, the challenge in assigning synaptic weights for the interconnection between neurons? How can this challenge be addressed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2388cb2",
   "metadata": {},
   "source": [
    "Learning Process In ANN:\n",
    "Learning process in ANN mainly depends on four factors, they are:\n",
    "\n",
    "The number of layers in the network (Single-layered or multi-layered)\n",
    "Direction of signal flow (Feedforward or recurrent)\n",
    "Number of nodes in layers: The number of node in the input layer is equal to the number of features of the input data set. The number of output nodes will depend on possible outcomes i.e. the number of classes in case of supervised learning. But the number of layers in the hidden layer is to be chosen by the user. A larger number of nodes in the hidden layer, higher the performance but too many nodes may result in overfitting as well as increased computational expense.\n",
    "Weight of Interconnected Nodes: Deciding the value of weights attached with each interconnection between each neuron so that a specific learning problem can be solved correctly is quite a difficult problem by itself. Take an example to understand the problem. Take the example of a Multi-layered Feed-Forward Network, we have to train an ANN model using some data, so that it can classify a new data set, say p_5(3,-2). Say we have deduced that p_1=(5,2)   and  p_2 = (-1,12)   belonging to class C1 while p_3=(3,-5)   and p_4 = (-2,-1)  belonging to class C2. We assume the values of synaptic weights w_0,w_1,w_2 as -2, 1/2 and 1/4 respectively. But we will NOT get these weight values for every learning problem. For solving a learning problem with ANN, we can start with a set of values for synaptic weights and keep changing those in multiple iterations. The stopping criterion may be the rate of misclassification < 1% or the maximum numbers of iterations should be less than 25(a threshold value). There may be another problem that, the rate of misclassification may not reduce progressively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14aa373e",
   "metadata": {},
   "source": [
    "Question 7:Explain, in details, the backpropagation algorithm. What are the limitations of this algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ca3699",
   "metadata": {},
   "source": [
    "Backward propagation: an error function measures how accurate the output of the network is. To improve the output, the weights have to be optimized. The backpropagation algorithm is used to determine how the individual weights have to be adjusted. The weights are adjusted during the gradient descent method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f48833",
   "metadata": {},
   "source": [
    "The actual performance of backpropagation on a specific problem is dependent on the input data.\n",
    "Back propagation algorithm in data mining can be quite sensitive to noisy data\n",
    "You need to use the matrix-based approach for backpropagation instead of mini-batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf04cf21",
   "metadata": {},
   "source": [
    "Question 8:Describe, in details, the process of adjusting the interconnection weights in a multi-layer neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e6c0ae",
   "metadata": {},
   "source": [
    "According to the descent algorithm, partial derivative of cost function E has to be taken with respect to interconnection weights. Mathematically it can be represented as:\n",
    "\n",
    "\n",
    "\\frac{\\partial E}{\\partial w^{'}_{jk}} = \\frac {\\partial}{\\partial w^{'}_{jk}}\\bigg\\{\\frac{1}{2}\\sum_{k=1}^{n}(t_k-f_z(z_{in\\_k})^2\\bigg\\}\n",
    "\n",
    "\n",
    "{Above expression is for the interconnection weights between the j-th neuron in the hidden layer and the k-th neuron in the output layer.} This expression can be reduced to \n",
    "\n",
    "\n",
    "\\frac{\\partial E}{\\partial w^{'}_{jk}}=-(t_k-z_{out\\_k})\\cdot f^{'}_z(z_{in\\_k}) \\cdot \\frac{\\partial}{\\partial w^{'}_{jk}}\\bigg\\{\\sum_{i=0}^ny_{out\\_i} \\cdot w_{ik}\\bigg\\}\n",
    "\n",
    "\n",
    "where, \n",
    "f^{'}_z(z_{in\\_k}) = \\frac{\\partial}{\\partial w^{'}_{jk}}(f_z(z_{in\\_k}))        or \\frac{\\partial E}{\\partial w^{'}_{jk}} = -(t_k-z_{out\\_k}) \\cdot f^{'}_z(z_{in\\_k}).y_{out\\_i}\n",
    "\n",
    "If we assume \\delta w_k = -(t_k-z_{out\\_k}) \\cdot f^{'}_z(z_{in\\_k})        as a component of the weight adjustment needed for weight  w_{jk}        corresponding to the k-th output neuron, then : \n",
    "\n",
    "\\frac{\\partial E}{\\partial w^{'}_{jk}}=\\delta w^{'}_k \\cdot y_{out\\_i}\n",
    "\n",
    "On the basis of this, the weights and bias need to be updated as follows: \n",
    "\n",
    "For weights: \\Delta w_{jk} =- \\alpha \\cdot \\frac{\\partial E}{\\partial w^{'}_{jk}}=- \\alpha \\cdot \\delta w^{'}_k \\cdot y_{out\\_i}\n",
    "Hence, w^{'}_{jk}(new)=w^{'}_{jk}(old) + \\Delta w^{'}_{jk}\n",
    "For bias: \\Delta w_{0k} = - \\alpha \\cdot \\delta w^{'}_k\n",
    "Hence, w^{'}_{0k} (new)=w'_{0k}(old)+\\Delta w'_{0k}\n",
    "In the above expressions, alpha is the learning rate of the neural network. Learning rate is a user parameter which decreases or increases the speed with which the interconnection weights of a neural network is to be adjusted. If the learning rate is too high, the adjustment done as a part of the gradient descent process may diverge the data set rather than converging it. On the other hand, if the learning rate is too low, the optimization may consume more time because of the small steps towards the minima. \n",
    "\n",
    "{All the above calculations are for the interconnection weight between neurons in the hidden layer and neurons in the output layer}\n",
    "\n",
    "Like the above expressions, we can deduce the expressions for â€œInterconnection weights between the input and hidden layers:\n",
    "\n",
    "For weights: \\Delta w_{ij} = -\\alpha \\cdot \\frac{\\partial E}{\\partial w_{ij}}=- \\alpha \\cdot \\delta w_j \\cdot x_{out\\_i}\n",
    "Hence, w_{ij}(new)=w_{ij}(old) + \\Delta w_{ij}\n",
    "For bias: \\Delta w_{0j} = - \\alpha \\cdot \\delta w_j\n",
    "Hence, w_{0j} (new)=w_{0j}(old)+\\Delta w_{0j}\n",
    "So, in this way, we can use the Backpropagation algorithm to solve various Artificial Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402c805e",
   "metadata": {},
   "source": [
    "Question 9:What are the steps in the backpropagation algorithm? Why a multi-layer neural network is required?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51314ef7",
   "metadata": {},
   "source": [
    "The backpropagation algorithm is used to train multilayer perceptrons. It propagates the error information from the end of the network to all the weights inside the network. It allows the efficient computation of the gradient or derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d3c69c",
   "metadata": {},
   "source": [
    "A multi layer perceptron can learn non linear functions that a single layer perceptron cannot learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e92da92",
   "metadata": {},
   "source": [
    "Question 10:Write short notes on:\n",
    "        \n",
    "1.\tArtificial neuron\n",
    "\n",
    "2.\tMulti-layer perceptron\n",
    "\n",
    "3.\tDeep learning\n",
    "\n",
    "4.\tLearning rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e636af34",
   "metadata": {},
   "source": [
    "An artificial neuron is a digital construct that seeks to simulate the behavior of a biological neuron in the brain. Artificial neurons are typically used to make up an artificial neural network â€“ these technologies are modeled after human brain activity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c772a3f",
   "metadata": {},
   "source": [
    "Multilayer perceptrons, also known as feedforward neural networks having two or more layers have a higher processing power.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b625464",
   "metadata": {},
   "source": [
    "Deep learning (also known as deep structured learning) is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22472744",
   "metadata": {},
   "source": [
    "In machine learning and statistics, the learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0315a8e0",
   "metadata": {},
   "source": [
    "Question 11:Write the difference between:-\n",
    "\n",
    "1.\tActivation function vs threshold function\n",
    "\n",
    "2.\tStep function vs sigmoid function\n",
    "\n",
    "3.\tSingle layer vs multi-layer perceptron\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32fcf67",
   "metadata": {},
   "source": [
    "1.The output is set at one of two levels, depending on whether the total input is greater than or less than some threshold value.\n",
    "\n",
    "Activation function translates the input signals to output signals. Four types of transfer functions are commonly used, Unit step (threshold), sigmoid, piecewise linear, and Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8d07d8",
   "metadata": {},
   "source": [
    "2.Binary step function is a threshold-based activation function which means after a certain threshold neuron is activated and below the said threshold neuron is deactivated. In the above graph, the threshold is zero. This activation function can be used in binary classifications as the name suggests, however it can not be used in a situation where you have multiple classes to deal with.\n",
    "\n",
    "\n",
    "Sigmoid function (also known as logistic function) takes a probabilistic approach and the output ranges between 0â€“1. It normalizes the output of each neuron. However, Sigmoid function makes almost no change in the prediction for very high or very low inputs which ultimately results in neural network refusing to learn further, this problem is known as the vanishing gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76e24b3",
   "metadata": {},
   "source": [
    "3.. Single-Layer Perceptrons\n",
    "\n",
    "Single-layer perceptrons can learn only linearly separable patterns.\n",
    "\n",
    " Multilayer Perceptrons\n",
    "\n",
    "Multilayer perceptrons, also known as feedforward neural networks having two or more layers have a higher processing power."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

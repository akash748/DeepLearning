{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5166e54b",
   "metadata": {},
   "source": [
    "Question 1: What are the advantages of a CNN over a fully connected DNN for image classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df61ea50",
   "metadata": {},
   "source": [
    "DNN works fine for small images (e.g., MNIST), it breaks down for\n",
    "larger images because of the huge number of parameters it\n",
    "requires. For example, a 100 × 100 image has 10,000 pixels, and if\n",
    "the first layer has just 1,000 neurons (which already severely\n",
    "restricts the amount of information transmitted to the next layer),\n",
    "this means a total of 10 million connections. And that’s just the first\n",
    "layer. CNNs solve this problem using partially connected layers and\n",
    "weight sharing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74abe62",
   "metadata": {},
   "source": [
    "Question 2: Consider a CNN composed of three convolutional layers, each with 3 × 3 kernels, a stride of 2, and \"same\" padding. The lowest layer outputs 100 feature maps, the middle one outputs 200, and the top one outputs 400. The input images are RGB images of 200 × 300 pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc4ea83",
   "metadata": {},
   "source": [
    "What is the total number of parameters in the CNN? If we are using 32-bit floats, at least how much RAM will this network require when making a prediction for a single instance? What about when training on a mini-batch of 50 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3aa28a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 100, 150, 400)     11200     \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 50, 75, 200)       720200    \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 25, 38, 100)       180100    \n",
      "=================================================================\n",
      "Total params: 911,500\n",
      "Trainable params: 911,500\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(filters = 400,input_shape = (200,300,3),kernel_size = (3,3),strides = (2,2),padding = 'same'))\n",
    "model.add(Convolution2D(filters = 200,kernel_size = (3,3),strides = (2,2),padding = 'same'))\n",
    "model.add(Convolution2D(filters = 100,kernel_size = (3,3),strides = (2,2),padding = 'same'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ee3b13",
   "metadata": {},
   "source": [
    "Amount of RAM Required is:\n",
    "\n",
    "32*200*300*400 = 96mb for 1 instance \n",
    "\n",
    "for 50 instance\n",
    "\n",
    "4.8GB of RAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07edd72",
   "metadata": {},
   "source": [
    "Question 3: If your GPU runs out of memory while training a CNN, what are five things you could try to solve the problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cd2e52",
   "metadata": {},
   "source": [
    "Five Ways are:\n",
    "\n",
    "Reducing mini batch size\n",
    "\n",
    "Using higher stride\n",
    "\n",
    "Removing few layers\n",
    "\n",
    "Try using 16-bit floats instead of 32-bit floats\n",
    "\n",
    "Distribute the CNN across multiple devices.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227dfcbd",
   "metadata": {},
   "source": [
    "Question 4: Why would you want to add a max pooling layer rather than a convolutional layer with the same stride"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aadb040",
   "metadata": {},
   "source": [
    "Reducing computations, memory usage and the number of parameters, a\n",
    "max pooling layer also introduces some level of invariance to small translations,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a462c9",
   "metadata": {},
   "source": [
    "By inserting a max pooling layer every few layers in\n",
    "a CNN, it is possible to get some level of translation invariance at a larger scale.\n",
    "Moreover, max pooling also offers a small amount of rotational invariance and a\n",
    "slight scale invariance. Such invariance (even if it is limited) can be useful in cases\n",
    "where the prediction should not depend on these details, such as in classification\n",
    "tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0484f9",
   "metadata": {},
   "source": [
    "Question 5: When would you want to add a local response normalization layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89513ee0",
   "metadata": {},
   "source": [
    "The most strongly activated\n",
    "neurons inhibit other neurons located at the same position in neighboring feature\n",
    "maps (such competitive activation has been observed in biological neurons). This\n",
    "encourages different feature maps to specialize, pushing them apart and forcing them to explore a wider range of features, ultimately improving generalization. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8ba041",
   "metadata": {},
   "source": [
    "Question 6: Can you name the main innovations in AlexNet, compared to LeNet-5? What about the main innovations in GoogLeNet, ResNet, SENet ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96781308",
   "metadata": {},
   "source": [
    "Gradual Improvements over LeNet-5:\n",
    "\n",
    "Alexnet - Added Local Response Normalization, Used ReLu Activation Function,trained on GPU\n",
    "\n",
    "Resnent - Introduced the concept of Deep Networks with the help of skip connections.\n",
    "\n",
    "GoogleNet - A 22 layer network which was also trained on breadth\n",
    "\n",
    "SENet - Introduced a SE Block for extreme performace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6a11a1",
   "metadata": {},
   "source": [
    "Question 7: What is a fully convolutional network? How can you convert a dense layer into a convolutional layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64e2c58",
   "metadata": {},
   "source": [
    "The idea of FCNs was first introduced in a 2015 paper23 by Jonathan Long et al., for\n",
    "semantic segmentation (the task of classifying every pixel in an image according to\n",
    "the class of the object it belongs to). They pointed out that you could replace the\n",
    "dense layers at the top of a CNN by convolutional layers. To understand this, let’s look\n",
    "at an example: suppose a dense layer with 200 neurons sits on top of a convolutional\n",
    "layer that outputs 100 feature maps, each of size 7 × 7 (this is the feature map size, not\n",
    "the kernel size). Each neuron will compute a weighted sum of all 100 × 7 × 7 activa‐\n",
    "tions from the convolutional layer (plus a bias term). Now let’s see what happens if we\n",
    "replace the dense layer with a convolution layer using 200 filters, each 7 × 7, and with\n",
    "VALID padding. This layer will output 200 feature maps, each 1 × 1 (since the kernel\n",
    "is exactly the size of the input feature maps and we are using VALID padding). In\n",
    "other words, it will output 200 numbers, just like the dense layer did, and if you look\n",
    "closely at the computations performed by a convolutional layer, you will notice that\n",
    "these numbers will be precisely the same as the dense layer produced. The only differ‐\n",
    "ence is that the dense layer’s output was a tensor of shape [batch size, 200] while the\n",
    "convolutional layer will output a tensor of shape [batch size, 1, 1, 200]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ccbc9b",
   "metadata": {},
   "source": [
    "To convert a dense layer to a convolutional layer, the number of fil‐\n",
    "ters in the convolutional layer must be equal to the number of units\n",
    "in the dense layer, the filter size must be equal to the size of the\n",
    "input feature maps, and you must use VALID padding. The stride\n",
    "may be set to 1 or more,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c11218",
   "metadata": {},
   "source": [
    "Question 8: What is data augmentation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d282eb",
   "metadata": {},
   "source": [
    "Data augmentation artificially increases the size of the training set by generating\n",
    "many realistic variants of each training instance. This reduces overfitting, making this\n",
    "a regularization technique. The generated instances should be as realistic as possible:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dca9b7",
   "metadata": {},
   "source": [
    "Question 9: What is the main technical difficulty of semantic segmentation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85efba73",
   "metadata": {},
   "source": [
    "The\n",
    "main difficulty in this task is that when images go through a regular CNN, they grad‐\n",
    "ually lose their spatial resolution (due to the layers with strides greater than 1): so a\n",
    "regular CNN may end up knowing that there’s a person in the image, somewhere in\n",
    "the bottom left of the image, but it will not be much more precise than that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3243b4fe",
   "metadata": {},
   "source": [
    "Question 10: Build your own CNN from scratch and try to achieve the highest possible accuracy on MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad3037bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "600/600 [==============================] - 231s 385ms/step - loss: 1.3859 - accuracy: 0.5166\n",
      "Epoch 2/10\n",
      "600/600 [==============================] - 248s 413ms/step - loss: 0.4906 - accuracy: 0.8512\n",
      "Epoch 3/10\n",
      "600/600 [==============================] - 249s 415ms/step - loss: 0.2669 - accuracy: 0.9325\n",
      "Epoch 4/10\n",
      "600/600 [==============================] - 250s 416ms/step - loss: 0.1673 - accuracy: 0.9635\n",
      "Epoch 5/10\n",
      "600/600 [==============================] - 248s 413ms/step - loss: 0.1346 - accuracy: 0.9704\n",
      "Epoch 6/10\n",
      "600/600 [==============================] - 249s 416ms/step - loss: 0.1120 - accuracy: 0.9747\n",
      "Epoch 7/10\n",
      "600/600 [==============================] - 257s 429ms/step - loss: 0.0933 - accuracy: 0.9798\n",
      "Epoch 8/10\n",
      "600/600 [==============================] - 266s 443ms/step - loss: 0.0814 - accuracy: 0.9817\n",
      "Epoch 9/10\n",
      "600/600 [==============================] - 255s 426ms/step - loss: 0.0688 - accuracy: 0.9850\n",
      "Epoch 10/10\n",
      "600/600 [==============================] - 250s 417ms/step - loss: 0.0726 - accuracy: 0.9840\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28db7adf640>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "from functools import partial\n",
    "from keras.datasets import mnist\n",
    "data = mnist.load_data()\n",
    "(X_train, y_train), (X_test, y_test) = data\n",
    "X_train = X_train.reshape(-1, 28, 28, 1)\n",
    "DefaultConv2D = partial(keras.layers.Conv2D,\n",
    " kernel_size=3, activation='relu', padding=\"SAME\")\n",
    "model = keras.models.Sequential([\n",
    " DefaultConv2D(filters=64, kernel_size=7, input_shape=[28, 28, 1]),\n",
    " keras.layers.MaxPooling2D(pool_size=2),\n",
    " DefaultConv2D(filters=128),\n",
    " DefaultConv2D(filters=128),\n",
    " keras.layers.MaxPooling2D(pool_size=2),\n",
    " DefaultConv2D(filters=256),\n",
    " DefaultConv2D(filters=256),\n",
    " keras.layers.MaxPooling2D(pool_size=2),\n",
    " keras.layers.Flatten(),\n",
    " keras.layers.Dense(units=128, activation='relu'),\n",
    " keras.layers.Dropout(0.5),\n",
    " keras.layers.Dense(units=64, activation='relu'),\n",
    " keras.layers.Dropout(0.5),\n",
    " keras.layers.Dense(units=10, activation='softmax'),\n",
    "])\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train,y_train,epochs = 10,batch_size = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "798f7d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.04858801141381264, 0.9883000254631042]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = X_test.reshape(-1, 28, 28, 1)\n",
    "model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a7010a",
   "metadata": {},
   "source": [
    "Awesome Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45164d5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38235acc",
   "metadata": {},
   "source": [
    "Questiion 1: What are the pros and cons of using a stateful RNN versus a stateless RNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543ee209",
   "metadata": {},
   "source": [
    "The benefits of using stateful RNNs are smaller network size ot lower training times. The disadvantage is that we are now responsible for training the network with a batch size that periodicity of the data and resetting the state after each epoch. In addition data should not be shuffled while training the network since the order in which the data is presented is relevant for stateful networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1e6eb7",
   "metadata": {},
   "source": [
    "Question 2: Why do people use Encoder–Decoder RNNs rather than plain sequence-to-sequence RNNs for automatic translation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7370d9ea",
   "metadata": {},
   "source": [
    "In general, if you translate a sentence one word at a time, the result will be terrible. For example, the French sentence “Je vous en prie” means “You are welcome,” but if you translate it one word at a time, you get “I you in pray.” Huh? It is much better to read the whole sentence first and then translate it. A plain sequence-to-sequence RNN would start translating a sentence immediately after reading the first word, while an encoder–decoder RNN will first read the whole sentence and then translate it. That said, one could imagine a plain sequence-to-sequence RNN that would output silence whenever it is unsure about what to say next (just like human translators do when they must translate a live broadcast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47128b9f",
   "metadata": {},
   "source": [
    "Question 3: How can you deal with variable-length input sequences? What about variable-length output sequences?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b54b240",
   "metadata": {},
   "source": [
    "To handle variable length input sequences, the simplest option is to set the sequence_length parameter when calling the static_rnn() or dynamic_rnn() functions. Another option is to pad the smaller inputs (e.g., with zeros) to make them the same size as the largest input (this may be faster than the first option if the input sequences all have very similar lengths). To handle variable-length output sequences, if you know in advance the length of each output sequence, you can use the sequence_length parameter (for example, consider a sequence-to-sequence RNN that labels every frame in a video with a violence score: the output sequence will be exactly the same length as the input sequence). If you don’t know in advance the length of the output sequence, you can use the padding trick: always output the same size sequence, but ignore any outputs that come after the end-of-sequence token (by ignoring them when computing the cost function)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcfefb2",
   "metadata": {},
   "source": [
    "Question 4: What is beam search and why would you use it? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c62c75",
   "metadata": {},
   "source": [
    "Beam search is an algorithm used in many NLP and speech recognition models as a final decision making layer to choose the best output given target variables like maximum probability or next output character. First used for speech recognition in 1976, beam search is used often in models that have encoders and decoders with LSTM or Gated Recurrent Unit modules built in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd129e3f",
   "metadata": {},
   "source": [
    "Question 5: What is an attention mechanism? How does it help?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a684def",
   "metadata": {},
   "source": [
    "In neural networks, Attention is a technique that mimics cognitive attention. The effect enhances some parts of the input while diminishing other parts - the thought being that the network should devote more focus to that small but important part of the data. Learning which part of the data is more important than others depends on the context and is trained by gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cc196e",
   "metadata": {},
   "source": [
    "Question 6: What is the most important layer in the Transformer architecture? What is its purpose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550125ee",
   "metadata": {},
   "source": [
    "The most important part here is the “Residual Connections” around the layers. This is very important in retaining the position related information which we are adding to the input representation/embedding across the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5980305",
   "metadata": {},
   "source": [
    "Question 7: When would you need to use sampled softmax?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38bf6bf",
   "metadata": {},
   "source": [
    "If your target vocabulary(or in other words amount of classes you want to predict) is really big, it is very hard to use regular softmax, because you have to calculate probability for every word in dictionary. By Using sampled_softmax_loss you only take in account subset V of your vocabulary to calculate your loss.\n",
    "\n",
    "Sampled softmax only makes sense if we sample(our V) less than vocabulary size. If your vocabulary(amount of labels) is small, there is no point using sampled_softmax_loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdca752f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
